## 常见名词

宿主机：Host/hypervisor

GPA：Guest 物理地址

GVA：Guest 虚拟地址

HPA：Host的物理地址，真正的物理地址

### 虚拟化中的网络

![img](https://img2018.cnblogs.com/blog/922925/201908/922925-20190801115735634-432153484.png)

在Linux中实现虚拟网络的方法中比较常用的工具有两个：bridge-utils 和 openvswitch，它们创建的虚拟网络设备是不能相互使用的，比如：bridge-utils创建的桥设备，openvswitch是无法识别的

## CPU虚拟化

#### 模拟

1. 模拟就是不管ring3还是ring0都需要进行翻译(Guest到Host的过程，因为Host才是运行硬件上的)。这种效率比较慢
#### 虚拟

1. 虚拟就是说，非特权的指令直接运行在os的ring3上，仅仅针对特权指令进行翻译。这种效率有所提升，因为只需要翻译特权指令。
2. 常见的方式

- 完全虚拟化：也就是宿主机不知道自己运行在虚拟机中，因此每次都是直接调用cpu指令。常见的为
  - BT：二进制翻译,一种软件实现的方式，其运行在ring1上，监听着guest中模拟的cpu发出来的指令。
  - HVM：硬件辅助的虚拟化，一种硬件的方式。此时cpu有5个环，多了一个-1环，guest中的cpu工作在ring0上，host的物理cpu工作在ring-1上。此时ring0没有执行特权指令的权限，他只是进行监听。
- 半虚拟化：各guest知道自己运行在虚拟机上，此时guest进行特权指令时会向host进行请求(也就是hyper call，而不是进行cpu指令的翻译)，而不是一股脑的进行cpu指令翻译。此时我们需要修改os内核。据说这种可以达到物理硬件的90到95%。半虚拟化会分为两段：前半段和后半段



## 内存虚拟化

进程：看到的是线性地址，简称为VA；

内核：看到的是物理地址，简称为PA。

比较复杂的是mmu。

#### MMU的虚拟化

- Intel：EPT技术，Extended Page Table
- AMD：NTP技术，Nested Page Table

![1581074168425](assets/1581074168425.png)

上图为虚拟机中mmu寻址过程，我们可以看到GVA通过虚拟的mmu转到HPA，节省了，GPA到HPA的转换，但是我们发现这个没用使用tlb，因为tlb的本质是缓存VA到PA的表。因此我们还需要虚拟化tlb

#### TLB的虚拟化

目的是避免虚拟机切换时，清空tlb缓存，因此引入了Tagged TLB技术。当然cpu要支持

- 对于内存，如果不支持mmu和tlb的硬件虚拟化，那么只能使用软件模拟，也就是[Shadow Page Table](http://blog.chinaunix.net/uid-14528823-id-4114242.html)和每次清空TLB。这样的话效率就很低下了。

## IO虚拟化

#### 模拟

完全使用软件来模拟真实硬件，比如网卡。

#### 半虚拟化

#### 透传(IO-through)

![1581156847781](assets/1581156847781.png)

Intel:VT-d技术，是基于北桥的硬件辅助的虚拟化技术。



### 虚拟化实现的两种方式

#### Type-II

在Host主机上，在安装虚拟机，比如VM Ware、kvm、virtualbox

#### Type-I

直接运行在硬件上。比如xen、vmware ESX

### Intel硬件辅助的虚拟化

- CPU：vt-x,EPT,tagged-TLB
- IO/CPU混合:IO-->vt-d,IOV

00:0C:29:0D:0A:F9

![1581239874895](assets/1581239874895.png)

![1581239748950](assets/1581239748950.png)



#### 细分虚拟化分类

![1581649638416](assets/1581649638416.png)



#### 我们可以使用命令行brctl的方式来实现：

需要注意的是:我们必须要启用network服务，而不是networkManager服务

1. 安装yum install bridge*
2. brctl addbr name如br0
3. ifconfig eth0/ens33 0 up(去除原来网卡上的ip)
4.  brctl addif br0 eth0
5. ifconfig br0 IP/NETMASK up
6. route add default gw GW
7. brctl stp br0 on:stp主要是检查当前是否存在环路的
8. ip link set dev br0 down/on:表示该设备是否启动

## xen虚拟化

### xen组成部分

![1581323861064](assets/1581323861064.png)

- Xen Hypervisor：Xen的虚拟化监视器(最核心的组成)，直接运行在硬件的基础上。提供给其他虚拟机接口。其主要的任务是：
  - CPU资源的分配
  - 内存资源的分配
  - 中断请求的分配
- Dom0:特权域，他是运行在Xen Hypervisor上的一个特殊的虚拟机。支持网络设备和块设备的半虚拟化。
  - 主要是用IO分配，因为xen只支持CPU和内存的虚拟化。Linux内核在3.0之后对关键特性进行了优化，可以运行在xen上。
  - 提供管理DomU工具栈。用于实现对虚拟机进行添加、启动、快照、停止、删除等操作。
- DomU：非特权域。根据其虚拟化方式实现，有多种实现方式。
  - PV：也就是Guest的半虚拟化技术
  - HVM：硬件辅助虚拟化(完全虚拟化)，此时IO也会进行完全虚拟化
  - PV ON HVM：CPU使用完全虚拟化，而IO使用半虚拟化

### Xen的PV技术

- 不依赖CPU的HVM特性，但要求GuestOS的内核做出修改以知晓自己运行于PV环境

  - 运行于DomU环境中的os：Linux(2.6.24+)、NetBSD、FreeBSD，win不行因为win无法修改内核

  ![1581328427740](assets/1581328427740.png)

### Xen的HVM技术

- 依赖Intel VT或AMD amd-v，还要依赖于QEMU来模拟IO设备。

  - 运行于DomU环境中的os:几乎所有支持次X86平台的，因为他不需要修改内核

  ![1581328492635](assets/1581328492635.png)

### Dom0和DomU配置不一样的

![1581336167197](assets/1581336167197.png)

![1581336199327](assets/1581336199327.png)

![1581336214837](assets/1581336214837.png)

### Xen的PV on HVM技术

- CPU运行在HVM模式下，IO设备运行在PV模式下
  - 运行于DomU中的os：只要os能驱动PV接口类型的IO设备

### Xen的工具栈/控制台

- xm/xend:在xen hypervisor的Dom0中要启动xend服务
  - xm：命令行管理工具，有诸多子命令：create/destroy/stop/pause

- xl:基于libxenlight提供的轻量级的命令行工具

- xe/xapi:提供了对xen管理的api，因此多用于cloud环境,常见的如Xen Server,XCP
- virsh/libvirt:做成了一个通用的工具

![1581329034068](assets/1581329034068.png)

### XenStore

- 为各Domain提供的共享信息存储空间，有着层级结构的名称空间，位于dom0

### Centos对Xen的支持

- RHEL5.7-：此时内核版本为2.6.18,默认的虚拟化技术为xen

- RHEL5.8-：支持xen和kvm
- RHEL 6+：以后只支持kvm，虽然不支持Dom0，但是支持DomU。

### 如何在Centos6.6上使用Xen

#### 方法一

- 编译3.0以上版本的内核，启动对Dom0的支持
- 编译xen

#### 方法二

- 制作好相关程序包的项目，主要有两个：
  - xen4centos
  - xen made easy

## KVM:Kernel-based Virtual Machine,Qumranet公司

### KVM的组件

主要分为两类：

- dev/kvm

  工作于hypervisor(也就是原始os)，在用户空间可通过ioctl()系统调用来完成VM创建、启动等管理功能。他是一个字符设备。

  - 功能

    创建VM、为VM分配内存、读写VCPU的寄存器、向VCPU注入中断、运行VCPU等等

- qemu

  工作于用户空间，主要用于实现模拟PC机器的IO设备

### KVM的特性

#### 内存管理

- 将分配个VM的内存交换至SWAP
- 支持使用Huge Page
- 支持使用Intel EPT或者AMD RVI技术完成内存地址映射：由原先的GVA--->VPA--->HPA,借助EPT/RVI技术GVA直接HPA转换
- 支持KSM（kernel same-page merging）

#### 硬件支持

- 取决于Linux内核

#### 存储

- 本地存储
- DAS/NAS
- 分布式存储(如 glusterfs)

#### 实时迁移

### KVM局限性

#### 一般局限性

- 最好是所有虚拟机的物理核心数不要超过物理核心总数

- 时间记录难以精确，依赖于时间同步机制

#### MAC地址

- VM量特别大时，存在冲突的可能性
- 实时迁移有众多限制
- 性能局限性

### KVM的工具栈

![1581432870825](assets/1581432870825.png)

- qemu
  - qemu-kvm
  - qemu-img
- libvirt
  - GUI:virt-manager,virt-viewer
  - CLI:virt-install,virsh

### virsh的本地模式和远程模式

![1581433189482](assets/1581433189482.png)

### QEMU

####  简介

qemu是一个广泛应用在开源计算机仿真器和虚拟机。

- 当作为仿真器时，可以在一种架构(如PC机)下运行另一种架构(如ARM)下的os和程序，而通过动态转化，其可以获得很高的运行效率；

- 当作为一个虚拟机时，qemu可以通过直接使用真机的系统资源，让虚拟资源能够获得接近于物理机的性能表现，qemu支持xen或者kvm模式下的虚拟化，当使用kvm时，qemu可以虚拟x85、服务器和嵌入式powerpc，以及s390的系统。

- qemu在运行于主机架构相同的目标架构时，可以使用kvm。例如当在一个x86兼容的处理器上运行qemu-system-x86时，可以利用KVM加速为宿主机和客户端提供更好的性能。


#### 主要用到以下几个部分

◇	标准选项；
◇	USB选项；
◇	显示选项；
◇	i386平台专用选项；
◇	网络选项；
◇	字符设备选项；
◇	蓝牙相关选项；
◇	Linux系统引导专用选项；
◇	调试/专家模式选项；
◇	PowerPC专用选项；
◇	Sparc32专用选项；qemu-kvm命令使用格式为“qemu-kvm  [options][disk_image]”，其选项非常多，不过，大致可分为如下几类。

##### qemu-kvm的标准选项

qemu-kvm的标准选项主要涉及指定主机类型、CPU模式、NUMA、软驱设备、光驱设备及硬件设备等。
◇	-name name：设定虚拟机名称；
◇	-M machine：指定要模拟的主机类型，如Standard PC、ISA-only PC或Intel-Mac等，可以使用“qemu-kvm -M ?”获取所支持的所有类型；
◇	-m megs：设定虚拟机的RAM大小；
◇	-cpu model：设定CPU模型，如coreduo、qemu64等，可以使用“qemu-kvm -cpu ?”获取所支持的所有模型；
◇	-smp n[,cores=cores][,threads=threads][,sockets=sockets][,maxcpus=maxcpus]：设定模拟的SMP架构中CPU的个数等、每个CPU的核心数及CPU的socket(插槽)数目等；PC机上最多可以模拟255颗CPU；maxcpus用于指定热插入的CPU个数上限；一个插槽socket对应一个物理cpu
◇	-numa opts：指定模拟多节点的numa设备；
◇	-fda file
◇	-fdb file：使用指定文件(file)作为软盘镜像，file为/dev/fd0表示使用物理软驱；
◇	-hda file
◇	-hdb file
◇	-hdc file
◇	-hdd file：使用指定file作为硬盘镜像；
◇	-cdrom file：使用指定file作为CD-ROM镜像，需要注意的是-cdrom和-hdc不能同时使用；将file指定为/dev/cdrom可以直接使用物理光驱；
◇	-drive option[,option[,option[,...]]]：定义一个硬盘设备；可用子选项有很多。
◇	file=/path/to/somefile：硬件映像文件路径；
◇	if=interface：指定硬盘设备所连接的接口类型，即控制器类型，如ide、scsi、sd、mtd、floppy、pflash及virtio等；
◇	index=index：设定同一种控制器类型中不同设备的索引号，即标识号；
◇	media=media：定义介质类型为硬盘(disk)还是光盘(cdrom)；
◇	snapshot=snapshot：指定当前硬盘设备是否支持快照功能：on或off；
◇	cache=cache：定义如何使用物理机缓存来访问块数据，其可用值有none、writeback、unsafe和writethrough四个；

- writeback
- writethrough：

		format=format：指定映像文件的格式，具体格式可参见qemu-img命令；
		-boot [order=drives][,once=drives][,menu=on|off]：定义启动设备的引导次序，每种设备使用一个字符表示；不同的架构所支持的设备及其表示字符不尽相同，在x86 PC架构上，a、b表示软驱、c表示第一块硬盘，d表示第一个光驱设备，n-p表示网络适配器；默认为硬盘设备；
	-boot order=dc,once=d，这个意思是重启之后d就不在其作用，这个目的是啥，比如你用光驱加载系统镜像进行安装，安装之后就不应该再让他启动了，否则一直死循环安装os了

##### qemu-kvm的显示选项

显示选项用于定义虚拟机启动后的显示接口相关类型及属性等。

◇	-nographic：默认情况下，qemu使用SDL来显示VGA输出；而此选项用于禁止图形接口，此时,qemu类似一个简单的命令行程序，其仿真串口设备将被重定向到控制台；
◇	-curses：禁止图形接口，并使用curses/ncurses作为交互接口；
◇	-alt-grab：使用Ctrl+Alt+Shift组合键释放鼠标；
◇	-ctrl-grab：使用右Ctrl键释放鼠标；
◇	-sdl：启用SDL，simple directMedia Layer,C语言开发，跨平台且开源多媒体程序文件；
◇	-spice option[,option[,...]]：启用spice远程桌面协议；其有许多子选项，具体请参照qemu-kvm的手册；
◇	-vga type：指定要仿真的VGA接口类型，常见类型有：
◇	cirrus：Cirrus Logic GD5446显示卡；
◇	std：带有Bochs VBI扩展的标准VGA显示卡；
◇	vmware：VMWare SVGA-II兼容的显示适配器；
◇	qxl：QXL半虚拟化显示卡；与VGA兼容；在Guest中安装qxl驱动后能以很好的方式工作，在使用spice协议时推荐使用此类型；
◇	none：禁用VGA卡；
◇	-vnc display[,option[,option[,...]]]：Virtual Network Computing，使用RFB(Remote FrameBuffer)协议远程控制另外主机。默认情况下，qemu使用SDL显示VGA输出；使用-vnc选项，可以让qemu监听在VNC上，并将VGA输出重定向至VNC会话；使用此选项时，必须使用-k选项指定键盘布局类型；其有许多子选项，具体请参照qemu-kvm的手册；

	display:
		（1）host:N
			172.16.100.7:1, 监听于172.16.100.7主的5900+N的端口上
		(2) unix:/path/to/socket_file
		(3) none
	
	options:
		password: 连接时需要验正密码；设定密码通过monitor接口使用change
		reverse: “反向”连接至某处于监听状态的vncview上；
	
	-monitor stdio：表示在标准输入输出上显示monitor界面
	-nographic
		Ctrl-a, c: 在console和monitor之间切换
		Ctrl-a, h: 显示帮助信息
	qemu-kvm -m 128 -smp 2 -name "test" -hda cirros-0.3.0-i386-disk.img -vnc 192.168.147.130:0 -monitor stdio
##### i386平台专用选项

◇	-no-acpi：禁用ACPI功能，GuestOS与ACPI出现兼容问题时使用此选项；
◇	-balloon none：禁用balloon设备；
◇	-balloon virtio[,addr=addr]：启用virtio balloon设备；

##### 网络属性相关选项

网络属性相关选项用于定义网络设备接口类型及其相关的各属性等信息。这里只介绍nic、tap和user三种类型网络接口的属性，其它类型请参照qemu-kvm手册。

◇	-net nic[,vlan=n][,macaddr=mac][,model=type][,name=name][,addr=addr][,vectors=v]：创建一个新的网卡设备并连接至vlan n中；PC架构上默认的NIC为e1000，macaddr用于为其指定MAC地址，name用于指定一个在监控时显示的网上设备名称；qemu可以模拟多个类型的网卡设备，如virtio、i82551、i82557b、i82559er、ne2k_isa、pcnet、rtl8139、e1000、smc91c111、lance及mcf_fec等；不过，不同平台架构上，其支持的类型可能只包含前述列表的一部分，可以使用“qemu-kvm -net nic,model=?”来获取当前平台支持的类型；nic主要用于管理网卡前半段
◇	-net tap[,vlan=n][,name=name][,fd=h][,ifname=name][,script=file][,downscript=dfile]：通过物理机的TAP网络接口连接至vlan n中，使用script=file指定的脚本(默认为/etc/qemu-ifup)来配置当前网络接口，并使用downscript=file指定的脚本(默认为/etc/qemu-ifdown)来撤消接口配置；使用script=no和downscript=no可分别用来禁止执行脚本；主要用于管理网卡后半段

- qemu-kvm -m 128 -smp 2 -name "test" -hda cirros-0.3.0-i386-disk.img  -net nic -net tap,name=xxf0.0,script=no，我们也可以指定脚本，脚本默认在/etc下
- /etc/qemu-ifup

[root@xiaxuefei etc]# cat qemu-ifup
#!/bin/bash
bridge=br0

if [ -n "$1" ]; then
  ip link set dev $1 up
  sleep 1
  brctl addif $bridge $1
  [ $? -eq 0 ] && exit 0 || exit 1
else
  echo "Error:no interface specified"
  exit 1
fi

- qemu-ifdown

[root@xiaxuefei etc]# cat qemu-ifdown
#!/bin/bash
bridge=br0

if [ -n "$1" ]; then
  ip link set dev $1 down
  brctl delif $bridge $1
  exit 0
else
  echo "Error:no interface specified"
  exit 1
fi

- 两个虚拟机内部通信时，注意mac地址不能相同,默认是相同的。

   

		-net user[,option][,option][,...]：在用户模式配置网络栈，其不依赖于管理权限；有效选项有：
		vlan=n：连接至vlan n，默认n=0；
		name=name：指定接口的显示名称，常用于监控模式中；
		net=addr[/mask]：设定GuestOS可见的IP网络，掩码可选，默认为10.0.2.0/8；
		host=addr：指定GuestOS中看到的物理机的IP地址，默认为指定网络中的第二个，即x.x.x.2；
		dhcpstart=addr：指定DHCP服务地址池中16个地址的起始IP，默认为第16个至第31个，即x.x.x.16-x.x.x.31；
		dns=addr：指定GuestOS可见的dns服务器地址；默认为GuestOS网络中的第三个地址，即x.x.x.3；
		tftp=dir：激活内置的tftp服务器，并使用指定的dir作为tftp服务器的默认根目录；
		bootfile=file：BOOTP文件名称，用于实现网络引导GuestOS；如：qemu -hda linux.img -boot n -net user,tftp=/tftpserver/pub,bootfile=/pxelinux.0

brctl addbr br0
brctl addif br0 eth0

brctl addbr br1

- 两个虚拟机内部通信时，注意mac地址不能相同,默认是相同的。

##### 隔离模型演示

qemu-kvm -m 128 -smp 2 -name "test1" -hda cirros-0.3.0-i386-disk.img  -net nic,macaddr=52:54:00:11:34:56 -net tap,ifname=xxf1.0,script=/etc/qemu-ifup
VNC server running on `::1:5901'

qemu-kvm -m 128 -smp 2 -name "test1" -hda cirros-0.3.0-i386-disk.img  -net nic,macaddr=52:54:00:12:34:56 -net tap,ifname=xxf0.0,script=/etc/qemu-ifup
VNC server running on `::1:5900'

登陆进去之后，进行配置ip

![1581582336256](assets/1581582336256.png)

##### NAT模型

NAT模型与路由模型的通信机制基本一致，唯一不同的就是，NAT模型可以使虚拟机直接与外部网络进行通信；即只需要在NAT模型中，在宿主机SNAT地址转换，将虚拟机的源地址IP转换为物理网卡的IP地址，发送报文至外部网络的主机；外部网络的主机收到报文后构建的响应报文的目标IP地址，为宿主机的物理网卡的IP地址，而后宿主机在将报文发送至虚拟机，实现了虚拟机与外部网络的通信。

##### 路由模型演示

- 注意：
  - 路由模式，要求我们需要使用ip link在物理机上添加一块物理网卡，一般接入桥上，一般在物理机上
  - ip link help查看命令的帮助
  - 路由模型中的虚拟机无法与外部进行通信；因为虚拟机发送到外部网络的主机时，源地址内网地址，内网地址无法与外部直接通信，且没有对源地址转换为物理网卡的IP地址，因此，外部网络发送的响应报文中的目标地址为虚拟机的IP地址时，无法到达宿主机的物理网卡，故虚拟机无法收到响应报文。

1. 创建两块虚拟网卡：ip link add veth1.0 type veth peer veth1.1

2. 激活这两块网卡(未激活时，需要使用ifconfig -a来查看)：

   [root@xiaxuefei etc]# ip link set dev veth0 up
   [root@xiaxuefei etc]# ip link set dev veth1.0 up

3. 我们将veth1.0添加到网桥上：[root@xiaxuefei etc]#brctl addif br0 veth1.0，剩下的一端放在物理机上

4. 给veth0添加一个地址：[root@xiaxuefei etc]# ifconfig veth0 192.168.2.222/24

5. 要ping通物理机，需要在虚拟机中配置网关：route add default gw  192.168.2.222

如果我们需要ping其他的主机，此时需要打开网络的转发功能。但是我们知道此时他是没法回包的，要想回包，我们可以使用nat服务，也就是配置iptables。

需要注意的是，这样的操作并不会成功。因为虚拟设备太多了。所以我们把veth这个虚拟网卡给去掉，直接用br0就行。

##### 桥接模型演示

- 注意：删除某块网卡上的地址：ip addr del  ip地址/子网掩码 dev 网卡设备名。

  桥接模型不同于隔离模型、路由模型和NAT模型；在模型下，宿主机会虚拟出一块网卡(真正处理到host的报文)作为该宿主机的通信网卡，而宿主机的物理网卡则成为桥接设备（也可称为交换机，**工作在混杂模式，也就是不管发给虚拟机中的guest还是host，都接下来，他会根据mac地址发现不同的主机**），此时，虚拟机相当于宿主机所在的局域网内单独的一台主机，它宿主机的地位是同等的，没有依存关系。

1. 安装yum install bridge*
2. brctl addbr name如br0
3. ip addr del 192.168.147.130 dev eth0;brctl addif  br0 eth0;ip addr add 192.168.147.130/24 dev br0;
4. ip link set dev br0 down/on:表示该设备是否启动

- 删除网卡上的某个ip:ip addr del IP/子网掩码 dev 设备名

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019030322495082.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2Njc2NoYW4=,size_16,color_FFFFFF,t_70)

使用ifconfig查看：

[root@xiaxuefei ~]# ifconfig
br0       Link encap:Ethernet  HWaddr 00:50:56:3C:0C:B2  
          inet addr:192.168.147.130  Bcast:0.0.0.0  Mask:255.255.255.0
          inet6 addr: fe80::f895:98ff:fe11:36a/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:764 errors:0 dropped:0 overruns:0 frame:0
          TX packets:24 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:0 
          RX bytes:27498 (26.8 KiB)  TX bytes:2280 (2.2 KiB)

eth0      Link encap:Ethernet  HWaddr 00:50:56:3C:0C:B2  
          inet addr:192.168.147.130  Bcast:192.168.147.255  Mask:255.255.255.0
          inet6 addr: fe80::250:56ff:fe3c:cb2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:32384 errors:0 dropped:0 overruns:0 frame:0
          TX packets:6938 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000 
          RX bytes:2848247 (2.7 MiB)  TX bytes:1219867 (1.1 MiB)

**发现原先的物理网卡的IP去掉了**。

5. 此时如果我们给虚拟机中的eth0添加一个192.168.147.x的ip，他应该是能和host进行通信的。

![1581651687205](assets/1581651687205.png)

###### KVM网络总结

1. 隔离模型：在host创建一个虚拟交换机(桥设备)，每个虚拟机的tap(后半段)设备直接添加至虚拟交换机上

   ![1581652077854](assets/1581652077854.png)

   br0,br1,br2都代表一个个的交换机。如果他们都在同一个网段，同一个网络，那么我们只需要用网线将br,b1,b2连接起来即可，也就是创建两个虚拟接口，串起来；

   但是如果他们不再一个网络，此时我们就需要借助一个路由器将他们连接起来，此时br0,br1,br2需要进行报文转发，此时我们就需要给他们配置地址，作为网关。

   ![1581652273303](assets/1581652273303.png)

   2. 路由模型：

      - 第一步：激活tap，并将其加入到指定的bridge
      - 第二步：给虚拟机的bridge添加地址，打开核心转发功能

   3. NAT模型

      - 第一步：激活tap，并将其加入到指定的bridge
      - 第二步：给虚拟机的bridge添加地址，打开核心转发功能，并添加nat规则

` nat脚本

 ## 添加脚本
[root@C65-201 ~]# vim /etc/qemu-natup 
#!/bin/bash
bridge="br-in"
net="192.168.1.0/24" // 网络地址
ifaddr=192.168.1.254 //物理网卡ip

checkbr() {
         if brctl show | grep -i "^$1"; then
                  return 0
         else
                  return 1
         fi
}

initbr() {
         brctl addbr $bridge
         ip link set $bridge up
         ip addr add $ifaddr dev $bridge
}

enable_ip_forward() {
         sysctl -w net.ipv4.ip_forward=1
}

setup_nat() {
         checkbr $bridge
         if [ $? -eq 1 ]; then
                  initbr
                  enable_ip_forward
                  iptables -t nat -A POSTROUTING -s $net ! -d $net -j MASQUERADE(地址伪装)
         fi
}

if [ -n "$1" ]; then
         setup_nat
         ip link set $1 up
         brctl addif $bridge $1
         exit 0
else
         echo "Error: no interface specified."
         exit 1
fi

## 删除脚本
[root@C65-201 ~]# vim /etc/qemu-natdown 
#!/bin/bash
bridge="br-in"
remove_rule() {
         iptables -t nat -F
}

isalone_bridge() {
         if ! brctl show | awk "/^$bridge/{print \$4}" | grep "[^[:space:]]" &> /dev/null; then
                  ip link set $bridge down
                  brctl delbr $bridge
                  remove_rule
         fi
}

if [ -n "$1" ];then
         ip link set $1 down
         brctl delif $bridge $1
         isalone_bridge
         exit 0
else
         echo "Error: no interface specified."
         exit 1
fi

 

## 保存后给该脚本添加执行权限并检查语法
[root@C65-201 ~]# chmod +x /etc/{qemu-natup,qemu-natdown}       # 添加执行权限
[root@C65-201 ~]# bash -n /etc/qemu-natup                       # 检查语法是否有误
[root@C65-201 ~]# bash -n /etc/qemu-natdown                     # 检查语法是否有误
`

   4. 桥接模型：缺点是内部的虚拟机直接暴露在互联网上

      - 第一步：激活tap，并将其加入到指定的bridge

<https://blog.51cto.com/jerry12356/2132221>

#### VLAN

1. VLAN有个数限制，4096个，因此我们需要VXLAN更牛逼的技术。
2. VLAN被称之为虚拟局域网，目的是为了隔离广播报文，划分不同的网络。我们知道路由器天生的可以干这事，但是路由器的接口太少。
3. 交换机属于二层设备，默认是不能隔离广播域的。

#### VLAN划分方法

- 基于MAC地址
- 基于交换机端口
- 基于IP实现
- 基于用户实现

#### 交换机的接口类型

- 访问链接

- 汇聚链接：就是两个物理交换机中的某个接口，无论什么vlan都会进行转发，到对端的交换机之后，能自动识别出来。

  ![1581999439122](assets/1581999439122.png)

#### linux内核支持vlan

- linux可以作为一个支持vlan的交换机
- 查看并启用vlan模块

![1582000419814](assets/1582000419814.png)

- 使用专门的配置工具来进行配置

  yum info vconfig

  yum install -y vconfig

   ls /proc/net/vlan/

  vconfig --help

- vlan间通信/路由

  - 通过路由器的方式，又分为两种
    - 利用访问链接机制：路由器为每个vlan提供一个接口
    - 利用汇聚链接：路由器只向交换机提供一个接口
  - 三层交换机：内部既有二层功能又有三层功能(提供路由的功能)

#### 注意：

1. 跨物理机的vlan技术，需要物理交换机也支持tagged vlan技术

#### gre

通用路由封装，是一种隧道技术，我们之前说lvs时候，提到过ip前面在加个ip，而gre不仅可以承载一个ip，还可以承载帧信息（因为我们知道物理帧是在本地网络通信的，而通过gre可以将这个物理帧传播到对端中去)，工作在ip层。

#### VXLAN

类似gre功能，先关闭gre，本身也支持隧道。

- 关闭gre

  ovs-vsctl del-port br-in gre0

- 创建一个新的端口

  ```shell
  [root@xiaxuefei ~]# ovs-vsctl add-port br-in vx0
  [root@xiaxuefei ~]# ovs-vsctl set interface vx0 type=vxlan options:remote_ip=192.168.20.2
  
  
  [root@xiaxuefei ~]# ovs-vsctl add-port br-in vx0
  [root@xiaxuefei ~]# ovs-vsctl set interface vx0 type=vxlan options:remote_ip=192.168.20.1
  
  
  
  [root@xiaxuefei ~]# ovs-vsctl list interface vx0
  _uuid               : e641df73-3272-4b57-a08d-f1f958f8771d
  admin_state         : up
  bfd                 : {}
  bfd_status          : {}
  cfm_fault           : []
  cfm_fault_status    : []
  cfm_flap_count      : []
  cfm_health          : []
  cfm_mpid            : []
  cfm_remote_mpids    : []
  cfm_remote_opstate  : []
  duplex              : []
  external_ids        : {}
  ifindex             : 0
  ingress_policing_burst: 0
  ingress_policing_rate: 0
  lacp_current        : []
  link_resets         : 0
  link_speed          : []
  link_state          : up
  mac                 : []
  mac_in_use          : "5e:b3:84:55:ea:27"
  mtu                 : []
  name                : "vx0"
  ofport              : 9
  ofport_request      : []
  options             : {remote_ip="192.168.20.2"}
  other_config        : {}
  statistics          : {collisions=0, rx_bytes=2968, rx_crc_err=0, rx_dropped=0, rx_errors=0, rx_frame_err=0, rx_over_err=0, rx_packets=32, tx_bytes=2968, tx_dropped=0, tx_errors=0, tx_packets=32}
  status              : {tunnel_egress_iface="eth4", tunnel_egress_iface_carrier=up}
  type                : vxlan
  
  ```


### KVM的安装

- 首先需要打开硬件虚拟化。

1. 确保CPU支持HVM

   egrep --color=auto '(vmx|svm)' /proc/cpuinfo，vmx是intel的，svm是amd的

2. 装载模块

   modprobe kvm
    modprobe kvm-intel

3. 验证

   ls /dev/kvm是否存在该文件

- 安装virt

  ![1581437024229](assets/1581437024229.png)

  -  rpm -ql qemu-kvm：查看是否安装成功，由于qemu-kvm 不是在path路径下，所以我们手动进行一个软连接：ln -sv /usr/libexec/qemu-kvm /usr/bin

### KVM虚拟机的创建

1. 下载磁盘文件

   https://github.com/cirros-dev/cirros/releases/download/0.3.0/cirros-0.3.0-i386-disk.img

2. 使用qemu-kuv工具启动虚拟

   qemu-kvm -m(内存大小) 128 -smp(CPU架构) 2(CPU个数) -name(虚拟机名字) "test" -hda(硬盘类型) cirros-0.3.0-i386-disk.img

   - 使用-hda来指定磁盘

   - 使用-drive来指定磁盘映像文件，这个参数更多

     qemu-kvm -m(内存大小) 128 -smp(CPU架构) 2(CPU个数) -name(虚拟机名字) "test"  -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio（这个是IO的半虚拟化）,media=disk,cache=writeback,format=qcow2

   - 通过cdrom启动winxp

     qemu-kvm -m(内存大小) 128 -smp(CPU架构) 2(CPU个数) -sockets=1,cores=2,threads=2  -name(虚拟机名字) "winxp"  -drive file=/images/kvm/winxp.img,if=ide,media=disk,cache=writeback,format=qcow2  -drive file=/images/kvm/winxp_ghost.iso,media=cdrom

     - winxp_ghost.iso是用来启动win的，然后将系统安装在第一个file的路径下的磁盘文件下，此时我们是可以进行分区的

3. 使用默认的VNC客户端来进行连接，注意这个VNC需要图形桌面的执行

      ss -tnl
      yum install tiger-vnc
     yum install tigervnc
     rpm -ql  tigervnc
     vncviewer :5900

   - 我们可以看到其实这个运行的虚拟机就是os上的一个进程，ps是可以看到的。只是这个虚拟机里面又分用户和内核模式

4. 登陆

qemu-kvm -m 128 -smp 2 -name "test" -hda cirros-0.3.0-i386-disk.img -vnc 192.168.147.130:0 -monitor stdio

 #### REHL实例

2.5.6.1.6 一个使用示例

下面的命令创建了一个名为rhel5.8的虚拟机，其RAM大小为512MB，有两颗CPU的SMP架构，默认引导设备为硬盘，有一个硬盘设备和一个光驱设备，网络接口类型为virtio，VGA模式为cirrus，并启用了balloon功能。

qemu-kvm -name "rhel5.8" -m 512 \

-smp 2 -boot d \
-drive file=/VM/images/rhel5.8/hda,if=virtio,index=0,media=disk,format=qcow2 \
-drive file=/isos/rhel-5.8.iso,index=1,media=cdrom \
-net nic,model=virtio,macaddr=52:54:00:A5:41:1E \
-vga cirrus -balloon virtio

需要注意的是，上述命令中使用的硬盘映像文件/VM/images/rhel5.8/hda需要事先使用qemu-img命令创建，其具体使用格式请见下节介绍。

在虚拟机创建并安装GuestOS完成之后，可以免去光驱设备直接启动之。命令如下所示。

qemu-kvm -name "rhel5.8" -m 512 \

-smp 2 -boot d \
-drive file=/VM/images/rhel5.8/hda,if=virtio,index=0,media=disk,format=qcow2 \
-net nic,model=virtio,macaddr=52:54:00:A5:41:1E \
-vga cirrus -balloon virtio



![1581841281174](assets/1581841281174.png)

 #### QUEM-KVM

qemu-kvm不能夸机器，因此我们可以通过agent来实现。

#### 通过PXE网络接口的方式来安装启动centos

![1581841205589](assets/1581841205589.png)

### qemu-img

- 使用qemu-img管理磁盘映像

qemu-img是qemu用来实现磁盘映像管理的工具组件，其有许多子命令，分别用于实现不同的管理功能，而每一个子命令也都有一系列不同的选项。其使用语法格式为“qemu-img  subcommand  [options]”，支持的子命令如下。

◇	create：创建一个新的磁盘映像文件；
◇	check：检查磁盘映像文件中的错误；
◇	convert：转换磁盘映像的格式；
◇	info：显示指定磁盘映像的信息；
◇	snapshot：管理磁盘映像的快照；
◇	commit：提交磁盘映像的所有改变；
◇	rbase：基于某磁盘映像创建新的映像文件；
◇	resize：增大或缩减磁盘映像文件的大小；

使用create子命令创建磁盘映像的命令格式为“create [-f fmt] [-o options] filename [size]”，例如下面的命令创建了一个格式为qcow2的120G的稀疏磁盘映像文件。

qemu-img create -f qcow2  /VM/images/rhel5.8/hda 120G

Formatting '/VM/images/rhel5.8/hda', fmt=qcow2 size=128849018880 encryption=off cluster_size=65536

### virtio半虚拟化

![1581841961788](assets/1581841961788.png)

- HVM：硬件辅助的虚拟化CPU
- IO半虚拟化分成两段

  - 前半段驱动(virtio前半段,出现在虚拟机中):virtio-blk,virtio-net,virtio-pci,virtio-balloon,virtio-console

  - 又分为三层:

    - virtio:虚拟队列，virt-ring,virtio如何区分后端多个虚拟机的报文呢？此时就需要一个环形队列。

    - transport:

    - 后端处理程序(virtio backend drivers):在qemu中实现
- virtio-balloon:动态调整虚拟机中的GuestOS的内存
  - qemu-kvm -balloon virtio:开启
  - 手动查看GuestOS的内存使用率
    - info balloon
    - balloon N

- virtio-net:其依赖于GuestOS中的驱动，及qemu中的后端程序

  - GuestOS:virtio_net.ko

  - Qemu:qemu-kvm -net nic,model=?

    如qemu-kvm -net nic,model=virtio

  - Host中的GSO，TSO：

    关掉可能会提升性能：

    ethtool -K $IF gso off

    ethtool -K $IF tso off

    ethtool -K $IF

  - vhost-net:用于取代工作于用户空间的qemu中为virtio-net实现的后端驱动(因为原先是使用qemu来实现的)以实现性能提升的驱动

    -net tap[,vnet_hdr=on|off]【,vhost=on|off]

    qemu-kvm -net tap,vnet_hdr=on,vhost=on
    

- virtio-blk:其依赖于GuestOS中的驱动，以及qemu中的后端驱动

- kvm-clock:半虚拟化的时钟

  grep -i "paravirt" /boot/config-2.6.32-504.el6.x86_64

  config_paravirt_guest=y

  config_paravirt=y

  config_paravirt_clock=y

- vm的迁移
  - 静态迁移
  - 动态迁移
  - 在待迁主机使用：
    - qemu-kvm -vnc :N -incoming tcp:0:7777(监听的端口号，下面要用的)
    - vncviewer :590N
  - 在源主机使用
    - 打开monitor接口，执行migrate tcp:目标主机_IP:端口号
  - 优点
    - 负载均衡
    - 对宿主机进行升级
    - 节约能源，比如省电。

### libvirtio工具

![1581850632058](assets/1581850632058.png)

yum -y install qemu-kvm libvirt python-virtinst virt-viewer virt-manager bridge-utils 

## 虚拟化网络

### linux内核

#### namespace

- 文件系统隔离
- 网络隔离：主要用于网络资源的隔离，包括网络设备(网卡)/ip地址，IP路由表，防火墙,/proc/net,/sys/class/net以及套接字等
- IPC隔离：
- 用户和用户组隔离：这个隔离性并不好，连docker都不用
- PID隔离：对名称空间内的PID重新编号，两个不同的名称空间可以使用相同的PID
- UTS隔离：Unix Time-sharing System，提供主机名称和域名的隔离

#### cgroup

- 用于完成资源配置，包括：
  - 限制被各namespace隔离起来的资源
  - 为资源设置权重、计算使用量、完成各种所需的管理任务等

#### 三个更强大的组件

- netns

- openvswitch

- gre：通用路由封装，是一种隧道技术，我们之前说lvs时候，提到过ip前面在加个ip，而gre不仅可以承载一个ip，还可以承载帧信息（因为我们知道物理帧是在本地网络通信的，而通过gre可以将这个物理帧传播到对端中去)，工作在ip层。


#### Linux Network Namespace专题--netns

- netns在内核实现，其控制功能由iproute所提供的netns这个OBJECT来提供

  centos6.6所提供的iproute不具有此OBJECT，需要依赖OpenStack Icehouse的EPEL源来提供

- 使用netns

  - ip netns list
  - ip netns add NAME
  - ip netns del NAME
  - ip netns exec NAME COMMAND

- 使用虚拟以太网卡

  ip link add FRONTEND-NAME type  veth peer name BACKEND-NAME

#### openvswitch简称ovs

##### 特性

1. 基于C语言研发的一款虚拟交换机软件，主要适用于虚拟化技术
2. 支持802.1q协议、trunk、access
3. 支持网卡绑定技术--NIC bonding,支持网卡的负载均衡
4. 支持NetFlow、sFlow，支持监控技术
5. 支持Qos的配置及策略
6. 支持GRE(通用路由封装技术)
7. 支持vxlan
8. 支持OpenFlow及其扩展技术
9. 支持基于linux内核完成高性能转发

##### 组成

- ovs-vswitched:后台服务进程,实现数据报文交换功能，和linux内核兼容模块一通实现基于流的交换技术
- ovsdb-server:ovs自带的数据库，主要保存了整个ovs的配置信息，例如接口，交换和vlan等
- ovs-dpctl
- ovs-vsctl：用于获取或更改ovs-switched的配置信息，其修改操作会保存至ovsdb-server中
- ovs-dbmonitor
- ovs-controller
- ovs-ofctl
- ovs-pki

##### 工具

- 安装openvswtich

yum install -y openvswitch*

rpm -ql openvswitch

service openvswitch start:启动服务

- 查看帮助

  ovs-vsctl --help

  ![1582445491113](assets/1582445491113.png)

- ovs-vsctl 命令使用

  - add-br NAME：添加桥设备，这个设备支持vxlan，vlan等高级功能的

  - show：查看创建的桥设备

  - list-br:简要显示所有已定义的桥

  - del-br name:删除某个桥

  - del-port BRIDGE PORT:从某个桥上移除指定的port

  - add-port BRIDGE PORT:将PORT添加至某个桥，此时ovs会自动给我们创建一个接口interface

    ![1582447612304](assets/1582447612304.png)

  - list-ports BRIDGE :显示指定bridge上已经添加的所有的port

  - list-ifaces BRIDGE:显示接口

- 显示ovs数据库中的数据，你可以将其理解为mysql的一行行的数据

  - 显示所有interface表

    ovs-vsctl  list Interface

  - 显示某个interface

    ovs-vsctl list Interface NAME

  - find查找命令，类似mysql的select语句

  - 

#### 具体案例

##### 基于单节点中kvm配置的内部网络模型1

![1582352447055](assets/1582352447055.png)

1. 安装qemu-kvm，并进行挂载

   - yum -y install qemu-kvm bridge-utils 
   - modinfo kvm
   - modprobe kvm

2. 搭建内网的两个kvm，让他们放在同一个交换机也就是桥上，通过脚本的方式：但是首先确保创建了br-in桥

   /etc/qemu-ifup

   ```shell
   [0 root@xiaxuefei /etc]# cat qemu-ifup
   #! /bin/bash
   #这个是内部使用的桥
   bridge=br-in
   brctl addbr br-in
   if [ -n "$1"]; then
     ip link set $1 up
   	brctl addif $bridge $1
   	[ $? -eq 0] && exit 0 || exit 1
   else
     echo "Error:no interface specified!"
   	exit 1
   fi
   
   ```

   给shell添加执行权限并进行验证

   ```shell
   [0 root@xiaxuefei /etc]# chmod +x qemu-ifup 
   [0 root@xiaxuefei /etc]# bash -n qemu-ifup
   ```

3. 启动两个kvm虚拟机：这次我们让他们在后台运行着

   ```shell
   虚拟机1：qemu-kvm -m 128 -smp 1 -name vm1 -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio,media=disk -net nic,macaddr=52:54:00:aa:bb:cc -net tap,ifname=vif1.0,script=/etc/qemu-ifup --nographic  --daemonize
   VNC server running on `::1:5900'
   
   虚拟机2：qemu-kvm -m 128 -smp 1 -name vm2 -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio,media=disk -net nic,macaddr=52:54:00:aa:bb:dd -net tap,ifname=vif1.1,script=/etc/qemu-ifup --nographic --daemonize
   VNC server running on `::1:5900'
   
   #因为我们需要进行配置，所以我们不能让他工作在后台，使用--nographic
   ```

![1582353734316](assets/1582353734316.png)

4. 创建路由器

   ip netns add r1

5. 先创建内部网的

   - 用于连接路由器到内部桥的一对网卡

     ip link add  rin-router type veth peer name rin-switch

   - 激活网卡

     ip link set  rin-router up

     ip link set   rin-switch up

   - 将switch那一头放到桥上，这一段不需要路由器

              brctl addif br-in rin-switch

   - 将router那一头放到路由器

     ip link set rin-router netns r1

   - 给路由器中的网卡也就是rin-router改个名字eth0，并激活

           ip netns exec r1 ip link set rin-router name eth0
           
           ip netns exec r1 ip link set   eth0 up

   - 给她一个内网地址

      ip netns exec r1 ifconfig eth0 10.0.1.254/24 up

   - 去两个kvm虚拟机中，分别设置ip地址为10.0.1.1/24和10.0.1.2/24

   - 将两个虚拟机的网关指向路由器那边的地址

     route add default gw  10.0.1.254

6. 创建外部网的

   - 用于连接路由器到外部桥的一对网卡

     ip link add  rex-router type veth peer name rex-switch

   - 激活网卡

     ip link set  rex-router up

     ip link set  rex-switch up

   - 将switch那一头放到物理桥上，这一段不需要路由器

            brctl addif br-ex rex-switch

   - 将router那一头放到路由器

       ip link set rex-router netns r1

   - 给路由器中的网卡也就是rex-router改个名字eth1，并激活

            ip netns exec r1 ip link set rex-router name eth1
            
            ip netns exec r1 ip link set   eth1 up

   - 给她一个外网地址，就是可以上网的

      ip netns exec r1 ifconfig eth1 192.168.147.150/24 up

7. 此时我们虚拟机的节点可以访问外网，但是呢，外网回不来，因为外网不知道有内网这个网站。此时我们需要给路由器进行nat配置。

   ip netns exec r1 iptables -t nat -A POSTROUTING -s 10.0.1.0/24 ! -d 10.0.1.0/24 -j SNAT --to-source 192.168.147.150

8. 如果两台虚拟机的ip是动态设置的，怎么办了？简单，我们只需要在路由器中运行一个dhcp服务器即可

        yum install -y dnsmasq

      ip netns exec r1 dnsmasq  --dhcp-range 10.0.1.100,10.0.1.120 --dhcp-option=option:router,10.0.1.254



##### 跨节点中的虚拟机通信

node1和node2,node3节点，我们不要求 node2,node3能访问外部网络，node1能对外连上网。要上网的话，只能通过node1来。

node2和node3属于计算节点

![1582371932314](assets/1582371932314.png)

1. node2和node3配置：

- 需要两块网卡

  新增的一块网卡，运行的模式，选择为仅主机模式。没其他可选的虚拟网络了

  ![1582370637361](assets/1582370637361.png)



- eth0:用于内部管理的

- eth4：用于两个节点内vm通信用的

- 修改网卡别名：

  /etc/udev/rules.d.70-persistent-net.rules

  modprobe -r e1000:移除原先的网卡模块

  modprobe e1000:重新加载进来

  - 小插曲，我们发现clone的两台主机，只有一台正常ssh连接上，另外一台死活连不上

    - 检查两者的物理地址是否相同

    - 检查ip是否相同

    - 都不行的话，交换一下两个名字

      ![1582373979909](assets/1582373979909.png)

- 此时我们给两个节点eth0分别配置ip

  node2:192.168.10.7

  node3:192.168.10.8

- 安装openvswtich

  yum install -y openvswitch*

  rpm -ql openvswitch

  service openvswitch start:启动服务

2. node1配置

   eth0:192.168.10.6,

   注意eth0只是内网命令用的，不需要网关

   node1的另外一个网卡eth4是连上互联网的。

3. node2和node3需要能上网，必须要把网关指向node1中的eth0，并且还需要配上iptables规则。这不就是一个nat模型吗？哈哈

   ![1582379372940](assets/1582379372940.png)

- service network restart一下

- 配置iptables规则：

   iptables -t nat -A POSTROUTING -s 192.168.10.0/24   -j SNAT --to-source 192.168.147.130(他是可以上网的)

   iptables -nL -v  -t nat

- 打开node1节点的ip转发功能

  vim  /etc/sysctl.conf

  ![1582380007744](assets/1582380007744.png)

```shell
[root@xiaxuefei ~]# sysctl -p
net.ipv4.ip_forward = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
kernel.sysrq = 0
kernel.core_uses_pid = 1
net.ipv4.tcp_syncookies = 1
kernel.msgmnb = 65536
kernel.msgmax = 65536
kernel.shmmax = 68719476736
kernel.shmall = 4294967296

```

- 记得关闭NetworkManager服务

  ```
  service NetworkManager stop   结束进程
  
  chkconfig NetworkManager off   关闭开机自启
  
  service network restart   重启网络服务
  ```

4. 启动kvm，注意此时我们不再用brctl来创建桥

   ```shell
   #确保创建了桥
   ovs-vsctl add-br br-in
   
   [root@xiaxuefei ~]# cat /etc/qemu-ifup
   #!/bin/bash
   bridge=br-in
   if [ -n "$1" ]; then
     ip link set dev $1 up
     sleep 1
     ovs-vsctl  add-port $bridge $1
     [ $? -eq 0 ] && exit 0 || exit 1
   else
     echo "Error:no interface specified"
     exit 1
   fi
   
   
   [root@xiaxuefei ~]# cat /etc/qemu-ifdown 
   #!/bin/bash
   bridge=br0
   
   if [ -n "$1" ]; then
     ip link set dev $1 down
     ovs-vsctl del-port $bridge $1
     exit 0
   else
     echo "Error:no interface specified"
     exit 1
   fi
   [root@xiaxuefei ~]# 
   
   
   虚拟机1：qemu-kvm -m 128 -smp 1 -name vm1 -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio,media=disk -net nic,macaddr=52:54:00:aa:bb:cc -net tap,ifname=vif0.0,script=/etc/qemu-ifup --nographic  --daemonize
   VNC server running on `::1:5900'
   
   虚拟机2：qemu-kvm -m 128 -smp 1 -name vm2 -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio,media=disk -net nic,macaddr=52:54:00:aa:bb:dd -net tap,ifname=vif0.1,script=/etc/qemu-ifup --nographic --daemonize
   VNC server running on `::1:5900'
   ```

   - 检查是否加上去，两个网卡是否也启动起来了

     ![1582449567992](assets/1582449567992.png)

     ![1582449598471](assets/1582449598471.png)

   - 给两个kvm虚拟机分别配置ip，进行ping

     vm1:ifconfig eth0 10.0.3.1/24 up

     vm2:ifconfig eth0 10.0.3.3/24 up

     ping能成功

5. 设置这两个kvm处于不同的vlan,在这里我们使用基于tag的方式来进行划分

   - ovs-vsctl list port：查看port这种表

   ![1582450116117](assets/1582450116117.png)

   - 修改tag

     ```shell
      #vlan的id是0到4095吧，默认为0，此时他们是ping不通的.要进行通信怎么办了？
      #1.添加路由
      #2.或者设置第二个也为10，在同一个vlan中
      ovs-vsctl set port vif0.0 tag=10
     ```

6. 跨交换机的不同vlan通信
- 在创建一个新的交换机

  ovs-vsctl add-br br-or

- 复制qemu-ifup和qemu-ifdown文件，以创建新的虚拟机

  ```shell
  [root@xiaxuefei etc]# cp qemu-ifup qemu-ifup2
  [root@xiaxuefei etc]# cp qemu-ifdown qemu-ifdown2
  ```

- 在创建一个虚拟机实例

  ```
  虚拟机3：qemu-kvm -m 128 -smp 1 -name vm3 -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio,media=disk -net nic,macaddr=52:54:00:aa:bb:ee -net tap,ifname=vif1.0,script=/etc/qemu-ifup2,downscript=/etc/qemu-ifdown2 --nographic
  ```

  ![1582451050285](assets/1582451050285.png)

- 配置ip地址

  vm3:ifconfig eth0 10.0.3.4/24 up

- 我们希望同一个节点上的vm3和vm1/2通信，ping不同。为啥了？

  因为他们不再同一个交换机上，这两个交换机没连起来

  - 手动连接两个交换机

    我们可以通过一对网卡

    ```shell
    #s0:第一个交换机
    #s1:第二个交换机
    ip link add s0 type veth peer name s1
    
    [root@xiaxuefei etc]# ip link set s0 up
    [root@xiaxuefei etc]# ip link set s1 up
    ```

  - 我们将这对网卡分别添加到对应交换机上,然后就可以ping通了

    ```shell
    [root@xiaxuefei etc]# ovs-vsctl add-port br-in s0 
    [root@xiaxuefei etc]# ovs-vsctl add-port br-or s1   
    ```

    ![1582454512644](assets/1582454512644.png)

  - 我们将vif0.0和vif1.0划分在一个vlan中(tag=10)，vif0.1在单独的vlan中(tag=0,默认值)

    ```shell
    [root@xiaxuefei etc]# ovs-vsctl set port vif0.0 tag=10
    [root@xiaxuefei etc]# ovs-vsctl set port vif1.0 tag=10 
    [root@xiaxuefei etc]# ovs-vsctl set port vif0.1 tag=0 
    ```

    此时同vlan下，可以ping通的。此时我们可是没管s0和s1哟。这是为啥呢？

    这是因为s0和s1默认工作在汇聚链路模式下。否则我们需要进行设置

##### 基于多节点中kvm配置的网络模型2--借助ovs

![1582362889342](assets/1582362889342.png)

![1582472108405](assets/1582472108405.png)

1. 我们每次都要为虚拟机配置ip地址，那么如何动态的创建呢？别忘记了，之前我们的dhcp服务器哟。

- 创建名称空间

  ```shell
  yum install -y iproute
  #创建名称空间，也就是路由器
  [root@xiaxuefei etc]# ip netns add r0
  #创建一对网卡，一半放在名称空间(路由器)，一半放在交换机
  [root@xiaxuefei etc]# ip link add sif0 type veth peer name rif0
  ##激活网卡
  [root@xiaxuefei etc]# ip link set sif0 up
  [root@xiaxuefei etc]# ip link set rif0 up
  #分别放到对应的位置
  [root@xiaxuefei etc]# ip link set rif0 netns r0
  [root@xiaxuefei etc]# ovs-vsctl add-port br-in sif0
  
  ```

- 激活路由器内部的网卡

  ```shell
  [root@xiaxuefei etc]# ip netns exec r0 ifconfig
  [root@xiaxuefei etc]# ip netns exec r0 ip link set rif0 up
  [root@xiaxuefei etc]# ip netns exec r0 ifconfig
  rif0      Link encap:Ethernet  HWaddr A2:AD:76:ED:A7:FE  
            inet6 addr: fe80::a0ad:76ff:feed:a7fe/64 Scope:Link
            UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
            RX packets:6 errors:0 dropped:0 overruns:0 frame:0
            TX packets:8 errors:0 dropped:0 overruns:0 carrier:0
            collisions:0 txqueuelen:1000 
            RX bytes:468 (468.0 b)  TX bytes:636 (636.0 b)
  
  #给路由器配置一个地址固定地址
  [root@xiaxuefei etc]# ip netns exec r0  ip addr add 10.0.4.254/24 dev rif0
  
  ```

- 在路由器内部运行一个dhcp服务器

  ```shell
  #86400表示租约的时间，为1天
  [root@xiaxuefei etc]# ip netns exec r0  dnsmasq -F 10.0.4.200,10.0.4.220,86400 -i rif0
  ```

- 启动一个虚拟机

  ```
  qemu-kvm -m 128 -smp 1 -name vm1 -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio,media=disk -net nic,macaddr=52:54:00:aa:bb:cc -net tap,ifname=vif0.0,script=/etc/qemu-ifup --nographic
  
  
  eth0:10.0.4.200
  ```

  ![1582463954399](assets/1582463954399.png)


1. 我们来配置第二个计算节点

- 可以单独配置一个dhcp服务器，也可以公用一个dhcp服务器
- 启动一个虚拟机，注意这个虚拟机的mac地址一定要不同,因为一会要通过gre隧道进行通信的

```shell
qemu-kvm -m 128 -smp 1 -name vm1 -drive file=/images/kvm/cirros-0.3.0-i386-disk.img,if=virtio,media=disk -net nic,macaddr=52:54:00:bb:bb:cc -net tap,ifname=vif0.0,script=/etc/qemu-ifup --nographic

eth0:10.0.4.220
```

2. 此时这两个kvm是不能通信的，因为两个交换机，并没有连接起来，虽然我们可以通过物理桥接的方式，但是这种方式，在现实中用的不多，代替的是使用gre隧道技术
3. 分别激活两个物理主机之前创建的网卡eth4（这个是物理接口）,我们使其具有gre功能（我们自己选择一个ip段就行）

```shell
node1:[root@xiaxuefei ~]# ifconfig eth4 192.168.20.1/24 up
node2:[root@xiaxuefei ~]# ifconfig eth4 192.168.20.2/24 up
```

4. 各自交换机上添加一个端口，并设置其接口类型为gre

![1582465439019](assets/1582465439019.png)

- 进行参数设置

  ```shell
  
  [root@xiaxuefei ~]# ovs-vsctl set interface  gre0 type=gre options:remote_ip=192.168.20.2(这个是对端地址，以后可以动态获取的)
  [root@xiaxuefei ~]# ovs-vsctl list interface gre0
  _uuid               : fd33c11b-a64d-4635-a35a-17a264c3cda1
  admin_state         : up
  bfd                 : {}
  bfd_status          : {}
  cfm_fault           : []
  cfm_fault_status    : []
  cfm_flap_count      : []
  cfm_health          : []
  cfm_mpid            : []
  cfm_remote_mpids    : []
  cfm_remote_opstate  : []
  duplex              : []
  external_ids        : {}
  ifindex             : 0
  ingress_policing_burst: 0
  ingress_policing_rate: 0
  lacp_current        : []
  link_resets         : 0
  link_speed          : []
  link_state          : up
  mac                 : []
  mac_in_use          : "9e:5f:e5:58:b5:24"
  mtu                 : []
  name                : "gre0"
  ofport              : 7
  ofport_request      : []
  options             : {remote_ip="192.168.20.2"}
  other_config        : {}
  #统计数据
  statistics          : {collisions=0, rx_bytes=0, rx_crc_err=0, rx_dropped=0, rx_errors=0, rx_frame_err=0, rx_over_err=0, rx_packets=0, tx_bytes=0, tx_dropped=0, tx_errors=0, tx_packets=0}
  #状态信息
  status              : {tunnel_egress_iface="eth4", tunnel_egress_iface_carrier=up}
  type                : gre
  
  
  ```

  另外一个节点

  ```shell
  [root@xiaxuefei ~]# ovs-vsctl add-port br-in gre0
  [root@xiaxuefei ~]# ovs-vsctl set interface gre0 type=gre options:remote_ip=192.168.20.1
  [root@xiaxuefei ~]# ping 192.168.20.2^C
  [root@xiaxuefei ~]# ovs-vsctl list interface gre0
  _uuid               : f5b1a02c-fa41-4a55-9e31-e8183a001ff5
  admin_state         : up
  bfd                 : {}
  bfd_status          : {}
  cfm_fault           : []
  cfm_fault_status    : []
  cfm_flap_count      : []
  cfm_health          : []
  cfm_mpid            : []
  cfm_remote_mpids    : []
  cfm_remote_opstate  : []
  duplex              : []
  external_ids        : {}
  ifindex             : 0
  ingress_policing_burst: 0
  ingress_policing_rate: 0
  lacp_current        : []
  link_resets         : 0
  link_speed          : []
  link_state          : up
  mac                 : []
  mac_in_use          : "ae:de:de:2d:ea:f0"
  mtu                 : []
  name                : "gre0"
  ofport              : 2
  ofport_request      : []
  options             : {remote_ip="192.168.20.1"}
  other_config        : {}
  statistics          : {collisions=0, rx_bytes=0, rx_crc_err=0, rx_dropped=0, rx_errors=0, rx_frame_err=0, rx_over_err=0, rx_packets=0, tx_bytes=0, tx_dropped=0, tx_errors=0, tx_packets=0}
  status              : {tunnel_egress_iface="eth4", tunnel_egress_iface_carrier=up}
  type                : gre
  
  ```

5. 重启两个物理节点上的kvm

   可以ping通了。

   node1-kvm1:ifconfig eth0 10.0.4.200/24 up

   node2-kvm1:ifconfig eth0 10.0.4.220/24 up

   在eth4上进行抓包：

   ![1582466557221](assets/1582466557221.png)

6. 我们可以通过vlan+gre技术，使得上图中的vm1和vm3通信，vm2和vm4通信，而vm1和vm2不能通信，vm3和vm4不能通信
